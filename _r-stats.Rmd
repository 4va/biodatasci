---
title: 'Essential Statistics with R'
---

This workshop will provide hands-on instruction and exercises covering basic statistical analysis in R. This will cover descriptive statistics, _t_-tests, linear models, chi-square, clustering, dimensionality reduction, and resampling strategies. We will also cover methods for "tidying" model results for downstream visualization and summarization. 

**Prerequisites: [Familiarity with R](r-basics.html) is _required_** (including working with [data frames](r-dataframes.html), installing/using packages, importing data, and saving results); familiarity with [dplyr](r-dplyr-yeast.html) and [ggplot2](r-viz-gapminder.html) packages is highly recommended. 

**You must [complete the basic R setup here](setup.html#R) _prior to class_.** This includes installing R, RStudio, and the required packages. Please [contact one of the instructors](people.html) _prior to class_ if you are having difficulty with any of the setup. Please bring your laptop and charger cable to class.


```{r, echo=FALSE, message=FALSE, eval=TRUE}
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE, cache=FALSE)
options(digits=3)
options(max.print=200)
.ex <- 1 # Track ex numbers w/ hidden var. Increment each ex: `r .ex``r .ex=.ex+1`
```


```{r make_data, include=FALSE, eval=FALSE}
###############################################################################
############# THIS CHUNK IS ONLY HERE TO SHOW HOW I MADE THE DATA #############
###############################################################################
library(dplyr)

# nhdemo <- Hmisc::sasxport.get("http://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT") %>% tbl_df 
# nhbody <- Hmisc::sasxport.get("http://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/BMX_G.XPT") %>% tbl_df
nhhiq <-  Hmisc::sasxport.get("http://wwwn.cdc.gov/nchs/nhanes/2011-2012/HIQ_G.XPT") %>% 
  transmute(ID=as.integer(seqn), Insured=as.integer(hiq011)) %>% 
  mutate(Insured=recode(Insured, `1`="Yes", `2`="No", .default=NA_character_, .missing=NA_character_))

nh <- NHANES::NHANES %>% filter(SurveyYr=="2011_12") %>% select(-SurveyYr) %>% inner_join(nhhiq, by="ID")

nh <- transmute(nh, 
                id=ID,
                Gender, 
                Age,
                Race=Race3, 
                Education, 
                MaritalStatus, 
                RelationshipStatus=if_else(MaritalStatus=="Married"|MaritalStatus=="LivePartner", "Committed", "Single", NA_character_),
                Insured,
                Income=HHIncomeMid,
                Poverty, 
                HomeRooms, 
                HomeOwn, 
                Work, 
                Weight, 
                Height, 
                BMI,
                Pulse,
                BPSys=BPSysAve, 
                BPDia=BPDiaAve, 
                Testosterone, 
                HDLChol=DirectChol, 
                TotChol,
                Diabetes,
                DiabetesAge,
                nPregnancies,
                nBabies,
                SleepHrsNight,
                PhysActive,
                PhysActiveDays,
                AlcoholDay,
                AlcoholYear,
                SmokingStatus = mosaic::derivedFactor(
                  Current = SmokeNow == "Yes",
                  Former = SmokeNow == "No",
                  Never  = Smoke100 == "No"
                ))

nh %>% readr::write_csv("data/nhanes.csv")
# nh <- readr::read_csv("data/nhanes.csv")
# nh <- nh %>% mutate_if(is.factor, funs(as.character))
# nhad <- nh %>% filter(Age >=18, BMI>1)

```


## Our data: NHANES

### About NHANES

The data we're going to work with comes from the National Health and Nutrition Examination Survey (NHANES) program at the CDC. You can read a lot more about NHANES on the [CDC's website](http://www.cdc.gov/nchs/nhanes/) or [Wikipedia](https://en.wikipedia.org/wiki/National_Health_and_Nutrition_Examination_Survey). NHANES is a research program designed to assess the health and nutritional status of adults and children in the United States. The survey is one of the only to combine both survey questions and physical examinations. It began in the 1960s and since 1999 examines a nationally representative sample of about 5,000 people each year. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The physical exam includes medical, dental, and physiological measurements, as well as several standard laboratory tests. NHANES is used to determine the prevalence of major diseases and risk factors for those diseases. NHANES data are also the basis for national standards for measurements like height, weight, and blood pressure. Data from this survey is used in epidemiology studies and health sciences research, which help develop public health policy, direct and design health programs and services, and expand the health knowledge for the Nation.

We are using a small slice of this data. We're only using a handful of variables from the 2011-2012 survey years on about 5,000 individuals. The CDC uses a [sampling strategy](http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) to purposefully oversample certain subpopulations like racial minorities. Naive analysis of the original NHANES data can lead to mistaken conclusions because the percentages of people from each racial group in the data are different from general population. The 5,000 individuals here are resampled from the larger NHANES study population to undo these oversampling effects, so you can treat this as if it were a simple random sample from the American population.

You can download the data at [the link above](data.html). The file is called [**nhanes.csv**](data/nhanes.csv). There's also a [data dictionary (filename **nhanes_dd.csv**)](data/nhanes_dd.csv) that lists and describes each variable in our NHANES dataset. This table is copied below.

```{r, echo=FALSE}
# library(dplyr)
# read.csv("data/nhanes_dd.csv", header=TRUE) %>% 
#   mutate(Variable=paste0("<strong>", Variable, "</strong>")) %>% 
#   # DT::datatable(rownames=FALSE, caption="NHANES Data Dictionary", escape=FALSE)
#   kable()
# haven't loaded libraries yet, keep it this way.
kable(transform(read.csv("data/nhanes_dd.csv", header=TRUE), Variable=paste0("<strong>", Variable, "</strong>")))
```


### Import & inspect

There are some built-in functions for reading in data in text files. These functions are _read-dot-something_ -- for example, `read.csv()` reads in comma-delimited text data; `read.delim()` reads in tab-delimited text, etc. We're going to read in data a little bit differently here using the [readr](https://github.com/hadley/readr) package. When you load the readr package, you'll have access to very similar looking functions, named _read-underscore-something_ -- e.g., `read_csv()`. You have to have the readr package installed to access these functions. Compared to the base functions, they're _much_ faster, they're good at guessing the types of data in the columns, they don't do some of the other silly things that the base functions do. We're going to use another package later on called [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html), and if you have the dplyr package loaded as well, and you read in the data with readr, the data will display nicely. 

First let's load those packages.

```{r loadpkgs}
library(readr)
library(dplyr)
```

If you see a warning that looks like this: `Error in library(packageName) : there is no package called 'packageName'`, then you don't have the package installed correctly. See the [setup page](setup.html). 

Now, let's actually load the data. You can get help for the import function with `?read_csv`. When we load data we assign it to a variable just like any other, and we can choose a name for that data. Since we're going to be referring to this data a lot, let's give it a short easy name to type. I'm going to call it `nh`. Once we've loaded it we can type the name of the object itself (`nh`) to see it printed to the screen. 

```{r loaddata}
nh <- read_csv(file="data/nhanes.csv")
nh
```

Take a look at that output. The nice thing about loading dplyr and reading in data with readr is that data frames are displayed in a much more friendly way. This dataset has 5,000 rows and 32 columns. When you import data this way and try to display the object in the console, instead of trying to display all 5,000 rows, you'll only see about 10 by default. Also, if you have so many columns that the data would wrap off the edge of your screen, those columns will not be displayed, but you'll see at the bottom of the output which, if any, columns were hidden from view. If you want to see the whole dataset, there are two ways to do this. First, you can click on the name of the data.frame in the **Environment** panel in RStudio. Or you could use the `View()` function (_with a capital V_).

```{r view, eval=FALSE}
View(nh)
```

Recall several built-in functions that are useful for working with data frames.

- Content:
    - `head()`: shows the first few rows
    - `tail()`: shows the last few rows
- Size:
    - `dim()`: returns a 2-element vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object)
    - `nrow()`: returns the number of rows
    - `ncol()`: returns the number of columns
- Summary:
    - `colnames()` (or just `names()`): returns the column names
    - `glimpse()` (from **dplyr**): Returns a glimpse of your data, telling you the structure of the dataset and information about the class, length and content of each column


```{r data_frame_functions, results="hide"}
head(nh)
tail(nh)
dim(nh)
names(nh)
glimpse(nh)
```


## Descriptive statistics

### Continuous variables

We can access individual variables within a data frame using the `$` operator, e.g., `mydataframe$specificVariable`. Let's print out all the **`Race`** values in the data. Let's then see what are the `unique` values of each. Then let's calculate the `mean`, `median`, and `range` of the **`Age`** variable.

```{r, results="hide"}
# Display all Race values
nh$Race

# Get the unique values of Race
unique(nh$Race)
length(unique(nh$Race))
# Do the same thing the dplyr way
nh$Race %>% unique()
nh$Race %>% unique() %>% length()

# Age mean, median, range
mean(nh$Age)
median(nh$Age)
range(nh$Age)
```


### Missing data

Let's try taking the mean of a different variable.

```{r}
mean(nh$Income)
```

What happened there? `NA` indicates _missing data_. Take a look at the Income variable.

```{r, results="hide"}
# Look at just the Income variable
nh$Income

# Or view the dataset
# View(nh)
```

Notice that there are lots of missing values for Income. Trying to get the mean a bunch of observations with some missing data returns a missing value by default. This is almost universally the case with all summary statistics -- a single `NA` will cause the summary to return `NA`. Now look at the help for `?mean`. Notice the `na.rm` argument. This is a logical (i.e., `TRUE` or `FALSE`) value indicating whether or not missing values should be removed prior to computing the mean. By default, it's set to `FALSE`. Now try it again.

```{r}
mean(nh$Income, na.rm=TRUE)
```

The `is.na()` function tells you if a value is missing. Get the `sum()` of that vector, which adds up all the `TRUE`s to tell you how many of the values are missing. 

```{r, results="hide"}
is.na(nh$Income)
sum(is.na(nh$Income))
```

There are a few handy functions in the [**Tmisc** package](https://cran.r-project.org/web/packages/Tmisc/index.html) for summarizing missingness in a data frame. You can graphically display missingness in a data frame as holes on a black canvas with **`gg_na()`** (use ggplot2 to plot `NA` values), or show a table of all the variables and the missingness level with **`propmiss()`**.

```{r TmiscLoad}
# Install if you don't have it already
# install.packages("Tmisc")

# Load Tmisc
library(Tmisc)
```

```{r ggna, fig.width=8}
gg_na(nh)
```

```{r}
propmiss(nh)
```

Now, let's practice.

----

<mark>**EXERCISE `r .ex``r .ex=.ex+1`**</mark>

Descriptive statistics functions for continuous variables:

- `mean()`
- `median()`
- `min()` and `max()`
- `range()`
- `quantile()`
- `sd()` and `var()`

1. fixme! Need to write these exercises!

----

### EDA

It's always worth examining your data visually before you start any statistical analysis or hypothesis testing. We could spend an entire day on **[exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis)**. The [data visualization lesson](r-viz-gapminder.html) covers this in much broader detail. Here we'll just mention a few of the big ones: **histograms** and **scatterplots**.

#### Histograms

We can learn a lot from the data just looking at the value distributions of particular variables. Let's make some histgrams with ggplot2. Looking at BMI shows a few extreme outliers. Looking at weight initially shows us that the units are probably in kg. Replotting that in lbs with more bins shows a clear bimodal distribution. Are there kids in this data? The age distribution shows us the answer is _yes_. 

```{r, include=FALSE}
library(ggplot2)
theme_set(theme_bw(base_size=16) + theme(strip.background = element_blank()))
```

```{r histograms}
library(ggplot2)
ggplot(nh, aes(BMI)) + geom_histogram()
ggplot(nh, aes(Weight)) + geom_histogram()
# In pounds, more bins
ggplot(nh, aes(Weight*2.2)) + geom_histogram(bins=80)
ggplot(nh, aes(Age)) + geom_histogram()
```


#### Scatterplots

Let's look at how a few different variables relate to each other. E.g., height and weight:

```{r scatter_heightweight}
ggplot(nh, aes(Height, Weight, col=Gender)) + geom_point()
```

Let's filter out all the kids, draw trend lines using a linear model:

```{r scatter_heightweight_colgender}
nh %>% 
  filter(Age>=18) %>% 
  ggplot(aes(Height, Weight, col=Gender)) + 
    geom_point() + 
    geom_smooth(method="lm")
```


Check out the [data visualization lesson](r-viz-gapminder.html) for much more on this topic.


## Continuous variables

### T-tests

First let's create a new dataset from `nh` called `nha` that only has adults.

```{r}
nha <- filter(nh, Age>=18)
```

Let's do a few two-sample t-tests to test for _differences in means between two groups_. The function for a t-test is `t.test()`. See the help for `?t.test`. We'll be using the _forumla_ method. The usage is **`t.test(response~group, data=myDataFrame)`**.

1. Are there differences in age for males versus females in this dataset?
2. Does BMI differ between diabetics and non-diabetics?
3. Do single or married/cohabitating people drink more alcohol? Is this relationship significant?

```{r}
t.test(Age~Gender, data=nha)
t.test(BMI~Diabetes, data=nha)
t.test(AlcoholYear~RelationshipStatus, data=nha)
```

See the heading, _Welch Two Sample t-test_, and notice that the degrees of freedom might not be what we expected based on our sample size. Now look at the help for `?t.test` again, and look at the `var.equal` argument, which is by default set to `FALSE`. One of the assumptions of the t-test is [homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity), or homogeneity of variance. This assumes that the variance in the outcome (e.g., BMI) is identical across both levels of the predictor (diabetic vs non-diabetic). Since this is rarely the case, the t-test defaults to using the [Welch correction](https://en.wikipedia.org/wiki/Welch%27s_t-test), which is a more reliable version of the t-test when the homoscedasticity assumption is violated.

### Wilcoxon test

Another assumption of the t-test is that data is normally distributed. Looking at the histogram for AlcoholYear shows that this data clearly isn't.

```{r alcoholhist}
ggplot(nha, aes(AlcoholYear)) + geom_histogram()
```

The [Wilcoxon rank-sum test (a.k.a. Mann-Whitney _U_ test)](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a nonparametric test of differences in mean that does not require normally distributed data. When data is perfectly normal, the t-test is uniformly more powerful. But when this assumption is violated, the t-test is unreliable. This test is called in a similar way as the t-test.

```{r wilcoxtest}
wilcox.test(AlcoholYear~RelationshipStatus, data=nha)
```

The results are still significant, but much less than the p-value reported for the (incorrect) t-test above.


### Linear models

> Analysis of variance and linear modeling are complex topics that deserve an entire semester dedicated to theory, design, and interpretation. A very good resource is [_An Introduction to Statistical Learning: with Applications in R_](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/ref=sr_1_1?ie=UTF8&qid=1473087847&sr=8-1&keywords=introduction+statistical+learning&tag=gettgenedone-20) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The [PDF](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) of the book and all the R code used throughout are [available **free** on the author's website](http://www-bcf.usc.edu/~gareth/ISL/). What follows is a necessary over-simplification with more focus on implementation, and less on theory and design.

Where t-tests and their nonparametric substitutes are used for assessing the differences in means between two groups, ANOVA is used to assess the significance of differences in means between multiple groups. In fact, a t-test is just a specific case of ANOVA when you only have two groups. And both t-tests and ANOVA are just specific cases of linear regression, where you're trying to fit a model describing how a continuous outcome (e.g., BMI) changes with some predictor variable (e.g., diabetic status, race, age, etc.). 

Let's examine the relationship between BMI and relationship status (`RelationshipStatus` was derived from `MaritalStatus`, coded as _Committed_ if MaritalStatus is Married or LivePartner, and _Single_ otherwise). Let's first do this with a t-test, and for now, let's assume that the variances between groups _are_ equal.

```{r}
t.test(BMI~RelationshipStatus, data=nha, var.equal=TRUE)
```

It looks like single people have a very slightly higher BMI than those in a committed relationship, but the magnitude of the difference is trivial, and the difference is not significant. Now, let's do the same test in a linear modeling framework. First, let's create the fitted model and store it in an object called `fit`. 

```{r}
fit <- lm(BMI~RelationshipStatus, data=nha)
```

You can display the object itself, but that isn't too interesting. You can get the more familiar ANOVA table by calling the `anova()` function on the `fit` object. More generally, the `summary()` function on a linear model object will tell you much more. 

```{r}
fit
anova(fit)
summary(fit)
```

Go back and re-run the t-test assuming equal variances as we did before. Now notice a few things:

1. The p-values from all three tests (t-test, ANOVA, and linear regression) are all identical (p=0.1256). This is because they're all identical: a t-test is a specific case of ANOVA, which is a specific case of linear regression. 
1. The test statistics are all related. The _t_ statistic from the t-test is **1.532**, which is the same as the t-statistic from the linear regression. If you square that, you get **2.347**, the _F_ statistic from the ANOVA. 
1. The `t.test()` output shows you the means for the two groups, Committed and Single. Just displaying the `fit` object itself or running `summary(fit)` shows you the coefficients for a linear model. Here, the model assumes the "baseline" RelationshipStatus level is _Committed_, and that the _intercept_ in a regression model (e.g., $\beta_{0}$ in the model $Y = \beta_{0} +  \beta_{1}X$) is the mean of the baseline group. Being _Single_ results in an increase in BMI of 0.3413. This is the $\beta_{1}$ coefficient in the model. You can easily change the ordering of the levels. See the help for `?factor`, and check out the new [**forcats** package](https://hadley.github.io/forcats/), which provides tools for manipulating **cat**egorical variables.

```{r}
# P-value computed on a t-statistic with 3552 degrees of freedom
# (multiply times 2 because t-test is assuming two-tailed)
2*(1-pt(1.532, df=3552))
# P-value computed on an F-test with 1 and 3552 degrees of freedom
1-pf(2.347, df1=1, df2=3552)
```

### ANOVA

Let's look at the relationship between smoking status (Never, Former, or Current), and BMI.

```{r}
fit <- lm(BMI~SmokingStatus, data=nha)
anova(fit)
summary(fit)
```

This tells us that there _is_ a significant difference in means between current, former, and never smokers (p=$4.54 \times 10^{-8}$). However, the linear model output might not have been what we wanted. Because the default handling of categorical variables is to treat the alphabetical first level as the baseline, "Current" smokers are treated as baseline, and this mean becomes the intercept, and the coefficients on "Former" and "Never" describe how those groups' means differ from current smokers. What if we wanted "Never" smokers to be the baseline, followed by Former, then Current? Have a look at `?factor` to relevel the factor levels.

```{r}
# Re-level the SmokingStatus variable
nha$SmokingStatus <- factor(nha$SmokingStatus, 
                            levels=c("Never", "Former", "Current"))

# Re-fit the model
fit <- lm(BMI~SmokingStatus, data=nha)

# Show the ANOVA table
anova(fit)

# Print the full model statistics
summary(fit)
```

Notice that the p-value on the ANOVA/regression didn't change, but the coefficients did. _Never_ smokers are now treated as baseline. The intercept coefficient (28.856) is now the mean for _Never_ smokers. The `SmokingStatusFormer` coefficient of .309 shows the apparent increase in BMI that former smokers have when compared to never smokers, but that difference is not significant (p=.24). The `SmokingStatusCurrent` coefficient of -1.464 shows that current smokers actually have a lower BMI than never smokers, and that this decrease is highly significant.

```{r smoking_boxplots}
ggplot(nha, aes(SmokingStatus, BMI)) + geom_boxplot() + theme_classic()
```

### Linear regression

Linear models are mathematical representations of the process that (_we think_) gave rise to our data. The model seeks to explain the relationship between a variable of interest, our _Y_, _outcome_, _response_, or _dependent_ variable, and one or more _X_, _predictor_, or _independent_ variables. Previously we talked about t-tests or ANOVA in the context of a simple linear regression model with only a single predictor variable, $X$:

$$Y = \beta_{0} +  \beta_{1}X$$

But you can have multiple predictors in a linear model that are all additive, accounting for the effects of the others:

$$Y = \beta_{0} +  \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon$$

- $Y$ is the response 
- $X_{1}$ and $X_{2}$ are the predictors
- $\beta_{0}, \beta_{1}, \beta_{2}$ are coefficients that describe what 1-unit changes in $X_{1}$ and $X_{2}$ do to the outcome variable $Y$.
- $\epsilon$ is random error. Our model will not perfectly predict $Y$. It will be off by some random amount. We assume this amount is a random draw from a Normal distribution with mean 0 and standard deviation $\sigma$.

_Building a linear model_ means we propose a linear model and then estimate the coefficients and the variance of the error term. Above, this means estimating $\beta_{0}, \beta_{1}, \beta_{2}$ and $\sigma$. This is what we do in R.

Let's look at the relationship between height and weight.

```{r}
fit <- lm(Weight~Height, data=nha)
summary(fit)
```

The relationship is highly significant (P<$2.2 \times 10^{-16}$). The intercept term is not very useful most of the time. Here it shows us what the value of Weight would be when Height=0, which could never happen. The Height coefficient is meaningful -- each one unit increase in height results in a 0.92 increase in the corresponding unit of weight. Let's visualize that relationship:

```{r scatter_height_weight_lm}
ggplot(nha, aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method="lm")
```

By default, this is only going to show the prediction over the range of the data. This is important! You never want to try to extrapolate response variables outside of the range of your predictor(s). For example, the linear model tells us that weight is -73.7kg when height is zero. We can extend the predicted model / regression line past the lowest value of the data down to height=0. The bands on the confidence interval tell us that the model is apparently confident within the regions defined by the gray boundary. But this is silly -- we would never see a height of zero, and predicting past the range of the available training data is never a good idea.

```{r scatter_height_weight_extrapolate}
ggplot(nha, aes(x=Height, y=Weight)) + 
  geom_point() + 
  geom_smooth(method="lm", fullrange=TRUE) + 
  xlim(0, NA) + 
  ggtitle("Friends don't let friends extrapolate.")
```

### Multiple regression

Finally, let's do a multiple linear regression analysis, where we attempt to model the effect of multiple predictor variables at once on some outcome. First, let's look at the effect of physical activity on testosterone levels. Let's do this with a t-test and linear regression, showing that you get the same results.

```{r}
t.test(Testosterone~PhysActive, data=nha)
summary(lm(Testosterone~PhysActive, data=nha))
```

In both cases, the p-value is significant (p=0.01516), and the result suggest that increased physical activity is associated with increased testosterone levels. Does increasing your physical activity increase your testosterone levels? Or is it the other way -- will increased testosterone encourage more physical activity? Or is it none of the above -- is the apparent relationship between physical activity and testosterone levels only apparent because both are correlated with yet a third, unaccounted for variable? Let's throw Age into the model as well.

```{r}
summary(lm(Testosterone~PhysActive+Age, data=nha))
```

This shows us that after accounting for age that the testosterone / physical activity link is no longer significant. Every 1-year increase in age results in a highly significant decrease in testosterone, and since increasing age is also likely associated with decreased physical activity, perhaps age is the third confounder that makes this relationship apparent.

Adding other predictors can also swing things the other way. We know that men have much higher testosterone levels than females. Sex is probably the single best predictor of testosterone levels in our dataset. By not accounting for this effect, our unaccounted-for variation remains very high. By accounting for Gender, we now reduce the residual error in the model, and the physical activity effect once again becomes significant. Also notice that our model fits much better (higher R-squared), and is much more significant overall.

```{r}
summary(lm(Testosterone~PhysActive+Age+Gender, data=nha))
```

We've only looked at the [`summary()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/summary.lm.html) and [`anova()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/anova.lm.html) functions for extracting information from an [`lm` class object](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.html). There are several other accessor functions that can be used on a linear model object. Check out the help page for each one of these to learn more.

- [`coefficients()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/coef.html)
- [`predict.lm()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/predict.lm.html)
- [`fitted.values()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/fitted.values.html)
- [`residuals()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/residuals.html)

## Discrete variables

Until now we've only discussed analyzing _continuous_ outcomes / dependent variables. We've tested for differences in means between two groups with t-tests, differences among means between _n_ groups with ANOVA, and more general relationships using linear regression. In all of these cases, the dependent variable, i.e., the outcome, or $Y$ variable, was _continuous_, and usually normally distributed. What if our outcome variable is _discrete_, e.g., "Yes/No", "Mutant/WT", "Case/Control", etc.? Here we use a different set of procedures for assessing significant associations.

### Contingency tables

The [`xtabs()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/xtabs.html) function is useful for creating contingency tables from categorical variables. Let's create a gender by diabetes status contingency table, and assign it to an object called **`xt`**. After making the assignment, type the name of the object to view it.

```{r}
xt <- xtabs(~Gender+Diabetes, data=nha)
xt
```

There are two useful funcitons, `addmargins()` and `prop.table()` that add more information or manipulate how the data is displayed. By default, `prop.table()` will divide the number of observations in each cell by the total. But you may want to specify _which margin_ you want to get proportions over. Let's do this for the first (row) margin.

```{r}
# Add marginal totals
addmargins(xt)

# Get the proportional table
prop.table(xt)

# That wasn't really what we wanted. 
# Do this over the first (row) margin only.
prop.table(xt, margin=1)
```

The chi-square test is used to assess the independence of these two factors. That is, if the null hypothesis that gender and diabetes are independent is true, the we would expect a proportionally equal number of diabetics across each sex. Males seem to be at slightly higher risk than females, but the difference is just short of statistically significant.

```{r}
chisq.test(xt)
```

An alternative to the chi-square test is [Fisher's exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test). Rather than relying on a critical value from a theoretical chi-square distribution, Fisher's exact test calculates the _exact_ probability of observing the contingency table as is. It's especially useful when there are very small _n_'s in one or more of the contingency table cells. Both the chi-square and Fisher's exact test give us p-values of approximately 0.06.

```{r}
fisher.test(xt)
```

There's a useful plot for visualizing contingency table data called a _mosaic_ plot. Call the `mosaicplot()` function on the contingency table object.

```{r}
mosaicplot(xt, main=NA)
```

Let's create a different contingency table, this time looking at the relationship between race and whether the person had health insurance. Display the table with marginal totals.

```{r}
xt <- xtabs(~Race+Insured, data=nha)
addmargins(xt)
```

Let's do the same thing as above, this time showing the proportion of people in each race category having health insurance.

```{r}
prop.table(xt, margin=1)
```

Now, let's run a chi-square test for independence.

```{r}
chisq.test(xt)
```

The result is _highly_ significant. In fact, so significant, that the display rounds off the p-value to something like $<2.2 \times 10^{-16}$. If you look at the help for [`?chisq.test`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/chisq.test.html) you'll see that displaying the test only shows you summary information, but other components can be accessed. For example, we can easily get the actual p-value, or the expected counts under the null hypothesis of independence.

```{r}
chisq.test(xt)$p.value
chisq.test(xt)$expected
```

We can also make a mosaic plot similar to above:

```{r, fig.width=8}
mosaicplot(xt, main=NA)
```

Finally, there's an _association plot_ that shows deviations from independence of rows and columns in a 2-dimensional contingency table. Each cell is represented by a rectangle that has (signed) height proportional to the deviation of the observed from expected counts and width proportional to the expected counts, so that the area of the box is proportional to the difference in observed and expected frequencies. The rectangles in each row are positioned relative to a baseline indicating independence. If the observed frequency of a cell is greater than the expected one, the box rises above the baseline and is shaded black; otherwise, the box falls below the baseline and is shaded red. See the help for [`?assocplot`](https://stat.ethz.ch/R-manual/R-patched/library/graphics/html/assocplot.html).


```{r, fig.width=8, fig.height=8}
assocplot(xt)
```

<!-- - xtabs eg. `xtabs(~Gender+Diabetes, data=nh)`, addmargins -->

<!-- `summary()`: works differently depending on what kind of object you pass to it. Passing a data frame to the `summary()` function prints out useful summary statistics about numeric column (min, max, median, mean, etc.) -->



### Logistic regression

```{r}
nha$Race <- relevel(factor(nha$Race), ref="White")
nha$Insured <- factor(nha$Insured)
fit <- glm(Insured~Race+Age+Income, data=nha, family='binomial')
summary(fit)
with(fit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```


## Power & sample size

```{r}
power.t.test(n=20, delta=2, sd=2.3)
power.t.test(power=.80, delta=.8, sd=1.5)
power.prop.test(n=5, p1=0.8, p2=0.1)
power.prop.test(power=0.8, p1=0.5, p2=0.2)
```


## Advanced topics

- Tidying models with **broom**
- Empirical Bayes
- Negative binomial
- Resampling/permutation

<!--

----

_Everything past this point is scratch_

----


## Scratch

- Jo Hardin's useR talk:

    - [useR talk on dynamic data in the classroom](https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/Dynamic-Data-in-the-Statistics-Classroom)
    - [repo on GitHub](https://github.com/hardin47/dynamicdata)
    - [slides](https://rawgit.com/hardin47/DynamicData/master/DynDatauseR.html)
- [Info on Framingham data](https://grimshaw-wiki.byu.edu/index.php/Risk_Factors_for_Cardiovascular_Disease_from_Framingham_Data)


```{r}
# Turn off evaluation of all further R chunks.
opts_chunk$set(eval=FALSE)
```


```{r}
library(dplyr)
library(ggplot2)

nhad <- nh %>% filter(Age >=18, BMI>1)
  
boxplot(BMI ~ RelationshipStatus, data=nhad, xlab="Relationship Status", ylab="BMI")

ggplot(nhad, aes(RelationshipStatus, BMI))+ geom_violin(color="orange")+ 
  xlab("Relationship Status") + ylab("BMI")

t.test(BMI ~ RelationshipStatus, data=nhad)
wilcox.test(BMI ~ RelationshipStatus, data=nhad)
anova(lm(BMI ~ RelationshipStatus, data=nhad))
summary(lm(BMI ~ RelationshipStatus, data=nhad))


ggplot(nhad, aes(x=Height, y=Weight, color=Gender)) + geom_point(alpha=.5)+ 
  xlab("Height") + ylab("Weight") + ggtitle("Height vs Weight by Gender")

last_plot() + geom_smooth(alpha=.1)
  
ggplot(nhad, aes(x=bmxht, y=bmxwt, group=gender, color=gender)) + 
  xlab("Height") + ylab("Weight") + geom_point(alpha=.5)+ 
  stat_smooth(alpha=1)+ 
  ggtitle("Height vs Weight by Gender with Smooth Regression Fit")

```



## t-tests

- `boxplot`
- `t.test`
- `power.t.test`
- welch correction
- wilcoxon test

## anova & linear models

- lm
- anova(fit)
- plot(fit)
- predict(fit, df)
- coefficients(fit)
- tidy(fit)


## Chi-square

- `xtabs %>% chisq.test`

```{r}
xtabs(~Race+Insured, data=nhad)
tab <- with(nhad, table(Race, Insured))
tab
addmargins(tab)
prop.table(tab)
prop.table(tab, margin=1)
chisq.test(tab)

with(nhad, table(Race, Insured)) %>% prop.table(margin=1)
```


## logistic regression

- `glm(..., family='binomial')`

```{r}
glm(factor(Insured)~Race, data=nhad, family='binomial') %>% tidy
glm(factor(Insured)~0+Race, data=nhad, family='binomial') %>% tidy
fit <- glm(factor(Insured)~Age+Race+Age*Race, data=nhad, family='binomial') 
summary(fit)
tidy(fit)
with(fit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
logLik(fit)
# http://www.r-bloggers.com/evaluating-logistic-regression-models/
fit2 <- glm(factor(Insured)~Race, data=nhad, family='binomial') 
with(fit2, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
anova(fit, fit2, test="Chisq")
library(lmtest)
lrtest(fit, fit2)
library(caret)
varImp(fit)
```

## Advanced

### Resampling

### Machine learning, classification, etc.


## From Hardin's Talk

https://github.com/hardin47/dynamicdata

## Introduction

The NHANES data come from the National Health and Nutrition Examination Survey, surveys given nationwide by the Center for Disease Controls (CDC). The data are collected to assess the health and well being of adults and children throughout the United States.  The survey is one of the only to combine both survey questions and physical examinations.

## Data information & loading data

We download data on demographic information and body image.  The data are in SAS format, but R has no trouble scraping the data from the NHANES website and uploading it into R.

```{r, include=FALSE, eval=FALSE}
library(Hmisc)
library(dplyr)
library(ggplot2)

nhdemo <- sasxport.get("http://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DEMO_G.XPT")
nhdemo <- mutate(nhdemo, gender = ifelse(nhdemo$riagendr==1, "male", "female")) 

nhbody <- sasxport.get("http://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/BMX_G.XPT")

nhbody <- sasxport.get("http://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/BMX_G.XPT")

nhhiq <-  sasxport.get("http://wwwn.cdc.gov/nchs/nhanes/2011-2012/HIQ_G.XPT") %>% 
  transmute(seqn=as.integer(seqn), hiq011=as.integer(hiq011)) %>% as_data_frame

comb <- inner_join(nhbody, nhdemo, by = "seqn")
```


Additionally, the NHANES data were collected using a cluster sampling scheme, so it is important to use the variables which describe the weights on the sampling to create a sample which is reflective of the population.  See the following for more information: http://web.grinnell.edu/individuals/kuipers/stat2labs/weights.html, ?NHANES (within R, using the NHANES packages), http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf.


```{r}
numobs = 2000
SRSsample <- sample(1:nrow(comb), numobs, replace=FALSE,
       prob=comb$wtmec2yr/sum(comb$wtmec2yr))
comb <- comb[SRSsample,]
```


## Using dynamic data within a typical classroom


One research question of interest is whether people in a committed relationship have a higher BMI than those who are not.  Note that a causal connection cannot be made here, but we are justified in thinking about the data as a good random sample from the US population.  We filter only the adults out of the sample and also created a relationship variable as to whether or not the individual is in a committed relationship.

The boxplots and violin plots both demonstrate that there is not a substantial difference between the BMI for those in committed relationships versus those who are not.  The tests of significance validate the ideas from the descriptive statistics.

It is worth noting here that the sample size is quite large.  If students repeat this analysis with different variables, it should be noted that very small effect sizes can be seen with large datasets.  A small p-value might indicate that there are significant effects, but an extra interpretation as to whether the effect is a practical difference warrants consideration.  It does not seem that the small *average* effect on BMI of being in a relationship is of any particular note when considering the large standard deviation across individual BMIs of both groups.

Additionally, again we point out that although these data are likely a good representation of the population, they cannot be used to find causal relationships.  Indeed, even if BMI had been different on average across the two groups, we do not know if lower BMI causes one to be more likely in a committed relationship or whether a committed relationship leads to a lower BMI.  Asking your students how one could gather such information would be a productive class discussion (e.g., paired observations, measurements over time, etc.).

```{r}
adults = comb %>% 
  filter(ridageyr >=18, bmxbmi>1) %>% 
  filter(dmdmartl>0 & dmdmartl < 10) %>% 
  mutate(rel=ifelse(dmdmartl==6|dmdmartl==1, "committed", "not")) %>%
  mutate(bmi=bmxbmi)
  
boxplot(bmi ~ rel, data=adults, xlab="Relationship Status", ylab="BMI")

ggplot(adults, aes(rel, bmi))+ geom_violin(color="orange")+ 
  xlab("Relationship Status") + ylab("BMI")

t.test(bmi ~ rel, data=adults)
dim(adults)
```


## Thinking outside the box

The data we have downloaded has many variables, some of which have meanings that are not immediately obvious.  The variable names are listed at the NHANES website, for example, the demographic data is at [http://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Demographics&CycleBeginYear=2011](http://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Demographics&CycleBeginYear=2011).

```{r}
names(adults)
```

For this analysis, however, we use height and weight.  We start with a simple scatterplot of height and weight with expected results (there is a correlation between height and weight, and men tend to be taller on average than women).  When adding a smoothed curve to the data, however, we are able to discuss how smooth curves are created, how to find the SE of the smooth curve, why there is extra variability due to extremes and also due to fewer data points on the ends, extrapolation (note that the two curves have different ranges), and the outcome that slopes of the two curves are not substantially different (no interaction) though might warrant further study.


```{r}
ggplot(adults, aes(x=bmxht, y=bmxwt, group=gender, color=gender)) + geom_point(alpha=.5)+ 
  xlab("Height") + ylab("Weight") + ggtitle("Height vs Weight by Gender")
  
ggplot(adults, aes(x=bmxht, y=bmxwt, group=gender, color=gender)) + 
  xlab("Height") + ylab("Weight") + geom_point(alpha=.5)+ 
  stat_smooth(alpha=1)+ 
  ggtitle("Height vs Weight by Gender with Smooth Regression Fit")
```


## Additional ideas for analysis:

With many continuous and categorical variables, the data can be used for both standard statistical regression (e.g., linear, logistic, etc.) or machine learning predictive modeling (e.g., LASSO, support vector machines, regression trees).


-->